
\documentclass[a4paper,11pt]{article}

% Chargement des packages %
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{color}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{listings}	% Pour la coloration syntaxique
\usepackage{graphicx}	% Pour les images


\title{Automatic documents classification\\
SortDC}
\author{Godefroy de Compreignac \and Ronan Letellier}
\date{July 5, 2011}



\begin{document}

% Titre du document %

\maketitle

\setcounter{page}{0}
\thispagestyle{empty}


% Table des matières %

\clearpage
\tableofcontents{}


% Sections %

\clearpage
\section*{Introduction}
\addcontentsline{toc}{section}{Introduction}

Nowadays, the always increasing amount of information available on the
Internet makes it impossible to let it be managed exclusively by humans. One
important remaining challenge in computer sciences is making it possible to
use computers to accomplish tasks that would normally require free will.
Indeed, some tasks can require a significant amount of time without producing
any added value. Automating some of these process would allow one to
concentrate on more complicated issues, thus gaining in productivity.

Our work during this semester (February 2011 - June 2011) consisted in finding
a mathematically correct algorithm in order to automatically sort
«~human-readable~» documents in different categories, and programming an
application using this algorithm to actually accomplish the task.



\section{Needs and goals}


\subsection{What did we want to achieve}

Our intention was to conceive an Open Source product that would be easy to
configure and use, as we could not find an existing one. To reach these
requirements, we had to be able to provide a RESTful\footnote{Representational
State Transfer} API\footnote{Application Programming Interface} that would allow anyone
to try and use our software features. To make it really flexible, we also
wanted it to work on different DBMS\footnote{Data Base Management System}.


\subsection{Why do we think this is useful}

While the internet content keeps growing, we need powerful devices to keep
some order in all human oriented items. Automatically understanding what a
text document is about allows search engines to determine wether a website
suits one's key words or not, our mailboxes to send junk mail directly in the
spam folder, and so on. However, such automated softwares are not available to
anyone, and we still have to order our own web contents by hand.

Working on server side and thanks to a RESTful API, the solution we offer
makes it possible for instance to categorize blogs articles, detect the
language of texts written by different users on a website or to determine
wether or not a comment or an inscription form is a spam.


\section{Studied technologies}

\subsection{Existing solutions}

We tried a couple of existing applications before programming our own :

\subsubsection{Apache Mahout}

Apache Mahout is an Open Source tool distributed by the Apache community.
It was designed to be scalable and provides implementations of multiple
algorithms used in machine learning such as K-means, Naive Bayes classifier,
key words, and so on.

Mahout is a Java application working on server side on Hadoop clusters. Its
usage is adapted for large scale calculations on several servers. Installing
it is not an easy task for one has to compile, install and configure Hadoop
and Mahout to make it work.


\subsubsection{RapidMiner}

RapidMiner\footnote{\url{http://rapid-i.com/content/view/181/190/lang,en/}} is
an Open Source application for machine learning, data mining, text
mining, predictive analytics and business analytics.
It is used for research, education, training, rapid prototyping, application
development, and industrial applications.

The Open Source version of RapidMiner, called «~Community Edition~», is very
limited (a lot less fonctionnalities than in its non-free version) and cannot
be used as a commercial application.

\subsection{Our selection}

We have not been able to find any Open Source software matching all our
expectations.


\section{Bayes classifier}

\subsection{Considerations}

Pour notre application, nous avons choisi d'utiliser le théorème de Bayes,
qui est le plus éprouvé et le moins consommateur de ressources serveur.

Les utilisations du théorème de Bayes peuvent fortement varier d'une
interprétation à une autre. Plusieurs interprétations peuvent donner des
résultats et des complexités de calcul très différentes.

Nous utilisons une approche probabiliste pour déterminer si un document
appartient à une catégorie ou à une autre. Pour cela, il faut choisir une
représentation du document que l'on peut analyser statistiquement. Un document
peut se décomposer en paragraphes, en phrases, en groupes de mots, en mots, en
n-grams de lettres, etc. On pourrait aussi introduire dans l'analyse les
positions relatives des mots par exemple.

Nous avons décidé de décomposer un document en «~tokens~» et de ne pas tenir
compte de leurs positions relatives. Les tokens peuvent être des mots, des
racines de mots (stemming), ou des n-grams de mots ou de lettres.


\subsection{Bayes' theorem}

La forme simple du théorème de Bayes, que nous allons utiliser, est :

\begin{equation}
    P(A| B) = \frac{P(B | A) P(A)}{P(B)}
\end{equation}


\subsection{Algorithm}

On considère:

$C = \{c_1, ..., c_{|C|}\}$ l'ensemble des catégories de notre classifier

$T = \{t_1, ..., t_{|T|}\}$ l'ensemble des tokens de tous les documents dans la base de données

$D = \{d_1, ..., d_{|D|}\}$ l'ensemble des documents dans la base de données

$\forall d_i \in D,\ d_i = \{t_1, ..., t_{|d_i|}\}$ l'ensemble des tokens d'un document $d_i$
~\\

On va définir une fonction $\phi : D \times C \rightarrow P$ qui va assigner une probabilité à chaque couple
$(d_i, c_i) \in D \times C$.

Un document étant en général unique, on ne peut pas lui appliquer directement
le théorème de Bayes. Il faut appliquer le théorème à ses tokens. Pour notre
algorithme, on va établir que la probabilité qu'a un document d'appartenir à
une catégorie est égale à la moyenne des probabilités qu'ont ses tokens
d'appartenir à la catégorie. On pose donc:

\begin{equation}
    \forall (d_i, c_i) \in D \times C,\ 
    \phi(d_i, c_i)
    = \sum_{t_j}^{|d_i|} (P(t_j | d_i) P(c_i | t_j))
\end{equation}

On applique le théorème de Bayes:

\begin{equation}
    \forall (c_i, t_j) \in C \times T,\ 
    P(c_i| t_j)
    = \frac{P(t_j | c_i) P(c_i)}{P(t_j)}
\end{equation}

On obtient:

\begin{equation}\label{eq:final_prob}
    \forall (d_i, c_i) \in D \times C,\ 
    \phi(d_i, c_i)
    = \sum_{t_j}^{|d_i|} \frac{P(t_j | d_i) P(t_j | c_i) P(c_i)}{P(t_j)}
\end{equation}

Pour calculer cela, on va avoir besoin de remplacer les probabilité par des
nombres d'occurrences de tokens. On pose:

$n_{tokens \in T} = |T|$ le nombre total de tokens différents (dans la base de
données).

$n_{t_i \in T}$ le nombre total d'occurrences du token $t_i$ (dans la base de
données).

$n_{tokens \in c_i}$ le nombre de tokens différents dans la catégorie $c_i$
(dans la base de données).

$n_{t_i \in c_i}$ le nombre d'occurrences du token $t_i$ dans la catégorie
$c_i$ (dans la base de données).

$n_{tokens \in d_i} = |d_i|$ le nombre de tokens différents dans le document $d_i$.

$n_{t_i \in d_i}$ le nombre d'occurrences du token $t_i$ dans le document
$d_i$.

L'équation (\ref{eq:final_prob}) donne donc:

\begin{eqnarray}\label{eq:final}
    \forall (d_i, c_i) \in D \times C,\ 
    \phi(d_i, c_i)
    &=& \sum_{t_j}^{|d_i|} (
        \frac{n_{t_j \in d_i}}{n_{tokens \in d_i}}
        \frac{n_{t_j \in c_i}}{n_{tokens \in c_i}}
        \frac{n_{tokens \in c_i}}{n_{tokens \in T}}
        \frac{n_{tokens \in T}}{n_{t_j \in T}}
    ) \nonumber \\
    &=& \sum_{t_j}^{|d_i|} (
        \frac{n_{t_j \in d_i}}{n_{tokens \in d_i}}
        \frac{n_{t_j \in c_i}}{n_{t_j \in T}}
    ) \nonumber \\
    &=& \frac{1}{n_{tokens \in d_i}}
        \sum_{t_j}^{|d_i|}
        \frac{n_{t_j \in d_i} n_{t_j \in c_i}}{n_{t_j \in T}}
\end{eqnarray}

L'équation (\ref{eq:final}) finale est relativement simple et directement
utilisable dans une application pour calculer la probabilité qu'a un document
d'appartenir à une catégorie.

Lors de l'entraînement de l'application, c'est-à-dire lorsqu'on assigne
manuellement des documents aux catégories, il suffira de stocker le nombre
d'occurrences de chaque token pour chaque catégorie (le nombre de total
d'occurrences d'un token peut alors être retrouvé par la sommes des nombres
d'occurrences dans les catégories).


\section{Our application}

\subsection{Quick overview}

\subsection{Features}

Sorting documents is not a one step process. We decided to go for texts
stemming and tokenization. Therefore, we tried to find existing solutions that
would help us to do so.

\subsubsection{Tokenization}

\subsubsection{Stemming}

Stemming seemed to be the hardest part, for we had to take in consideration
that texts may be in different languages. Finding a word's root depends on
text language and for we cannot speak whole of them, using a pre-existing
solution appeared to be the best solution to us.

We needed stemming for our algorithm is based on tokens probabilities.
Therefore, words could not be considered independantly for it may distort
the results. Indeed, the application had to understand that the word "cooked"
and the word "cooking" belong to the same category for instance.

Various open source applications use stemming, but a lot of them use it for
a specific purpose in a specific language (for instance, Sphinx
full-text-search). Snowball Stemmer, on the other hand, has specialized in
stemming text documents only and provides a very complete set of supported
languages (more than 15 latin languages).

\subsubsection{Tokenization}

Concerning tokenization, the work seemed a lot more doable, and as we
thought, we couldn't find any interesting tokenization application. We
consequently decided to programm our own tokenization class.
Tokenization allows us to split a text into "tokens" (words that have
been stemmed and cut) that will be used to determine in which category the
document has to be sorted. It is possible to chose tokens maximum length,
stop words (words that may not be significants, such as common articles or
pronouns for exemple) or words minimum length (to exclude short prepositions
for instance).

\subsubsection{RESTful API}

\subsubsection{DBMS}


\subsection{Some tests}
    
We tested our application on a set of 20 000 pre-sorted text documents. We
first trained 18 997 documents from that corpus then tried to categorize
accuratly the remaining 1000. Here are the results of these tests :

With Mysql:

Training: 186 mins (about 102 documents per minute)
Sorting: 10 mins 11 s

With MongoDB:

Training: 128 mins (about 148 documents per minute)
Sorting: 6 mins 21 s

The accuracy rate for the sorting part was 81.5\%.

\section*{Conclusion}
\addcontentsline{toc}{section}{Conclusion}


\clearpage
\section*{Bibliography}
\addcontentsline{toc}{section}{Bibliography}


\end{document}
