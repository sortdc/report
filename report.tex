
\documentclass[a4paper,11pt]{article}

% Chargement des packages %
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{color}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{listings}	% Pour la coloration syntaxique
\usepackage{graphicx}	% Pour les images


\title{Automatic documents classification\\
SortDC}
\author{Godefroy de Compreignac \and Ronan Letellier}
\date{July 5, 2011}



\begin{document}

% Titre du document %

\maketitle

\setcounter{page}{0}
\thispagestyle{empty}


% Table des matières %

\clearpage
\tableofcontents{}


% Sections %

\clearpage
\section*{Introduction}
\addcontentsline{toc}{section}{Introduction}

Nowadays, the always increasing amount of information available on the
Internet makes it impossible to let it be managed exclusively by humans. One
important remaining challenge in computer sciences is making it possible to
use computers to accomplish tasks that would normally require free will.
Indeed, some tasks can require a significant amount of time without producing
any added value. Automating some of these process would allow one to
concentrate on more complicated issues, thus gaining in productivity.

Our work during this semester (February 2011 - June 2011) consisted in finding
a mathematically correct algorithm in order to automatically sort
«~human-readable~» documents in different categories, and programming an
application using this algorithm to actually accomplish the task.



\section{Needs and goals}


\subsection{What did we want to achieve}

Our intention was to conceive an Open Source product that would be easy to
configure and use, as we could not find an existing one. To reach these
requirements, we had to be able to provide a RESTful\footnote{Representational
State Transfer} API\footnote{Application Programming Interface} that would allow anyone
to try and use our software features. To make it really flexible, we also
wanted it to work on different DBMS\footnote{Data Base Management System}.


\subsection{Why do we think this is useful}

While the internet content keeps growing, we need powerful devices to keep
some order in all human oriented items. Automatically understanding what a
text document is about allows search engines to determine wether a website
suits one's key words or not, our mailboxes to send junk mail directly in the
spam folder, and so on. However, such automated softwares are not available to
anyone, and we still have to order our own web contents by hand.

Working on server side and thanks to a RESTful API, the solution we offer
makes it possible for instance to categorize blogs articles, detect the
language of texts written by different users on a website or to determine
wether or not a comment or an inscription form is a spam.


\section{Studied technologies}

\subsection{Existing solutions}

We tried a couple of existing applications before programming our own, mainly:

\subsubsection{Apache Mahout}

Apache Mahout\footnote{\url{http://mahout.apache.org/}} is an Open Source tool
distributed by the Apache community.
It was designed to be scalable and provides implementations of multiple
algorithms used in machine learning such as K-means, Naive Bayes classifier,
key words, and so on.

Mahout is a Java application working on server side on Hadoop clusters. Its
usage is adapted for large scale calculations on several servers. Installing
it is not an easy task for one has to compile, install and configure Hadoop
and Mahout to make it work.


\subsubsection{RapidMiner}

RapidMiner\footnote{\url{http://rapid-i.com/content/view/181/190/lang,en/}} is
an Open Source application for machine learning, data mining, text
mining, predictive analytics and business analytics.
It is used for research, education, training, rapid prototyping, application
development, and industrial applications.

The Open Source version of RapidMiner, called «~Community Edition~», is very
limited (a lot less fonctionnalities than in its non-free version) and cannot
be used as a commercial application.

\subsubsection{GATE}

GATE\footnote{\url{http://gate.ac.uk/}} is an open source software «~capable
of solving almost any text processing problem~». It is a powerful text-mining
tool, but hard to install and configure if we want a classification server
with an API.

\subsubsection{Textimate}

Textimate\footnote{\url{http://textimate.me/}} is a webservice offering an API
allowing user to train and request classifiers to categorize texts and detect
spams. The source code is non accessible (not an Open Source software) and one
has to pay for more than 500 requests per month.

\subsubsection{uClassify}

uClassify\footnote{\url{http://www.uclassify.com/}} is a free webservice
offering an API allowing user to train and request classifiers just as
textimate does. Some applications of this software are gender, age or mood
detection of a text author.

Though using the API is free, the source code is once again hidden and works
exclusively on the Windows platform. uClassify offers non free licenses for
the installation of a private server.

\subsection{Feedback}

We haven't been able to find any Open Source software matching all our
expectations. The ones we found were too complicated, too heavy or did not
match the Open Source requirement.

We therefore decided to work on our own application, our aim being providing
a free, intuitive, easy to configure application as light as possible. Our
very first concern has then been to find an efficient classification
algorithm.


\section{Bayes classifier}

\subsection{Considerations}

We chose Bayes' theorem for our application for it is the most used method for
classification programms and it consumes a lot less server ressources.

Bayes' theorem usages can strongly vary depending on its interpretation.
Consequently, the calculations complexity and the results can be very
different.

We are using a probability approach to determine wether a document belongs to
a category or another. To that end, we need a representation of the document
that we can analyse statisticly. A document can be broken down into
paragraphs, sentences, groups of words, n-grams letters, etc. We also could
consider in the analysis words relative positions to each other for instance.

We decided to split a document in «~tokens~» without taking account of their
relative positions. Tokens can be words, words roots (stemming) or n-grams
words or letters.

\subsection{Bayes' theorem}

Bayes' theorem simple form, that we will be using, is the following :

\begin{equation}
    P(A| B) = \frac{P(B | A) P(A)}{P(B)}
\end{equation}


\subsection{Algorithm}

Considering:

$C = \{c_1, ..., c_{|C|}\}$ our classifier's categories

$T = \{t_1, ..., t_{|T|}\}$ all tokens from all documents stored in the database

$D = \{d_1, ..., d_{|D|}\}$ all documents stored in the database

$d_i = \{t_1, ..., t_{|d_i|}\}$ all tokens from a document $d_i$
~\\

Let's define the function $\phi : D \times C \rightarrow P$ that will assign
a probability to each couple
$(d_i, c_i) \in D \times C$.

A document being most of the time unique, we cannot apply Bayes' theorem
directly. We have to apply it to its tokens. For our algorithm, we will
establish that the probability that a document has to belong to a category
equals the average probability that its tokens have to belong to that
category. Thus, we have:

\begin{equation}
    \forall (d_i, c_i) \in D \times C,\ 
    \phi(d_i, c_i)
    = \sum_{j=1}^{|d_i|} (P(t_j | d_i) P(c_i | t_j))
\end{equation}

We apply Bayes' theorem:

\begin{equation}
    \forall (c_i, t_j) \in C \times T,\ 
    P(c_i| t_j)
    = \frac{P(t_j | c_i) P(c_i)}{P(t_j)}
\end{equation}

We get:

\begin{equation}\label{eq:final_prob}
    \forall (d_i, c_i) \in D \times C,\ 
    \phi(d_i, c_i)
    = \sum_{j=1}^{|d_i|} \frac{P(t_j | d_i) P(t_j | c_i) P(c_i)}{P(t_j)}
\end{equation}

To calculate this, we will need to replace probabilities with tokens number of
occurences. We have:

$n_{tokens \in T} = |T|$ the total number of different tokens stored in the
database.

$n_{t_i \in T}$ the total number of occurence of token $t_i$ stored in the
database.

$n_{tokens \in c_i}$ the number of different tokens in category $c_i$ stored
in the database.

$n_{t_i \in c_i}$ the number of occurences of token $t_i$ in category $c_i$
in the database.

$n_{tokens \in d_i} = |d_i|$ the number of different tokens in document $d_i$.

$n_{t_i \in d_i}$ the number of occurences of token $t_i$ in document $d_i$.

The equation (\ref{eq:final_prob}) leads to:

\begin{eqnarray}\label{eq:final}
    \forall (d_i, c_i) \in D \times C,\ 
    \phi(d_i, c_i)
    &=& \sum_{j=1}^{|d_i|} (
        \frac{n_{t_j \in d_i}}{n_{tokens \in d_i}}
        \frac{n_{t_j \in c_i}}{n_{tokens \in c_i}}
        \frac{n_{tokens \in c_i}}{n_{tokens \in T}}
        \frac{n_{tokens \in T}}{n_{t_j \in T}}
    ) \nonumber \\
    &=& \sum_{j=1}^{|d_i|} (
        \frac{n_{t_j \in d_i}}{n_{tokens \in d_i}}
        \frac{n_{t_j \in c_i}}{n_{t_j \in T}}
    ) \nonumber \\
    &=& \frac{1}{n_{tokens \in d_i}}
        \sum_{j=1}^{|d_i|}
        \frac{n_{t_j \in d_i} n_{t_j \in c_i}}{n_{t_j \in T}}
\end{eqnarray}

The final equation (\ref{eq:final}) is pretty simple and directly usable in
an application to calculate the probabilty that a document has to belong to
a category.

During the application training phase, we manually assign documents to
categories. We will then store the number of occurences of each token in each
category (the total number of a token's occurences can then be found as the
sum of occurences in categories).

\section{Our application}

\subsection{Quick overview}

We named our application SortDC (Document Classification). It is an Open Source
Java software, available here: \url{http://github.org/sortdc/sortdc}\\
It can be used to classify web contents (blog articles, news, etc.), detect
spams in messages or inscription forms, detect a text language, and so on.\\
Here are some features.

\subsection{Features}

Sorting documents is not a one step process. We decided to go for texts
stemming and tokenization.

In order to make our programm easy to use or try, we designed a RESTful API.
As said previously, we wanted it to be user friendly and portable, that's why
we made it DMBS compliant, making it possible to use it with MySQL databases
as well as MongoDB ones, and provided a YAML\footnote{"YAML Ain't Markup
Language" is a serialization language}
configuration file so that the application behavior can be easily modified.

\subsubsection{Stemming}

Stemming seemed to be the hardest part, for we had to take in consideration
that texts may be in different languages. Finding a word's root depends on
text language and for we cannot speak whole of them, using a pre-existing
solution appeared to be the best solution to us.

We needed stemming for our algorithm is based on tokens probabilities.
Therefore, words could not be considered independantly for it may distort
the results. Indeed, the application had to understand that the word "cooked"
and the word "cooking" belong to the same category for instance.

Various open source applications use stemming, but a lot of them use it for
a specific purpose in a specific language (for instance, Sphinx
full-text-search). Snowball\footnote{\url{http://snowball.tartarus.org/}},
on the other hand, has specialized in
stemming text documents only and provides a very complete set of supported
languages (more than 15 latin languages).

\subsubsection{Tokenization}

Concerning tokenization, the work seemed a lot more doable, and as we
thought, we couldn't find any interesting tokenization application. We
consequently decided to programm our own tokenization class.

Tokenization allows us to split a text into "tokens" (words that have
been stemmed and cut) or n-grams words or letters (a set of n words or
letters) that will be used to determine in which category the document has to
 be sorted. It is possible to chose tokens maximum length, stop words (words
that may not be significants, such as common articles or pronouns for
exemple) or words minimum length (to exclude short prepositions for
instance).

\subsubsection{RESTful API}

In order to make the application reachable and usable by any website or
programm whatever the language used, we designed a REST API that respects
RESTfull recommandations and accepts JSON and XML data.

We chose the Jersey\footnote{\url{http://jersey.java.net/}} library as a
web server directly integrated to our application to provide the API.
~\\

The API URLs are shaped as followed:\\
http://localhost:1337/classifiers \\
http://localhost:1337/classifiers/\{classifier\_id\} \\
http://localhost:1337/classifiers/\{classifier\_id\}/documents \\
http://localhost:1337/classifiers/\{classifier\_id\}/documents/\{document\_id\} \\
http://localhost:1337/classifiers/\{classifier\_id\}/categories \\
http://localhost:1337/classifiers/\{classifier\_id\}/categories/\{category\_id\} \\
http://localhost:1337/classifiers/\{classifier\_id\}/categories/\{category\_id\}/documents \\
~\\

GET, POST, PUT and DELETE methods allow the user to access, add, modify and
delete classifiers, categories and documents. Input and output formats are
given by \textit{Content-Type} and
\textit{Accept} headers.

To calculate categories probabilities for an untrained document, we simply
call \\ http://localhost:1337/classifiers/\{classifier\_id\}/categories with the
GET method and give the text in text/plain, XML or JSON format right after the
HTTP headers.

\subsubsection{YAML configuration and DBMS}

The whole application behavior can be managed through one simple YAML file.
It offers the possibility to configure webservice host and port, log verbose
and a path to a log file and classifiers features. In this classifier section
one can indicate the classifier's name and language, its DBMS as well as
database infos (username, password, host, port), wether or not it should be
using stemming, wether or not it should use words as tokens,  words minimum
length to be included in the classification process, tokens maximum length,
the kind of tokens it should extract (n-grams words or characters) and a list
or path to a file listing different stop words.

The application is currently compatible with MySQL\footnote{\url{http://www.mysql.com/}}
(InnoDB) and MongoDB\footnote{\url{http://www.mongodb.org/}}. We chose to use
MongoDB for its flexibility and scalability.

\subsection{Some tests}
    
We tested our application on a set of 20 000 pre-sorted text documents
\footnote{20 Newsgroups data set: \url{http://people.csail.mit.edu/jrennie/20Newsgroups/}}.
We first trained 18 997 documents from that corpus then tried to categorize
accuratly the remaining 1000. Here are the results of these tests :

With Mysql:\\
Training: 186 mins (about 102 documents per minute)\\
Sorting: 10 mins 11 s

With MongoDB:\\
Training: 128 mins (about 148 documents per minute)\\
Sorting: 6 mins 21 s

The accuracy rate for the sorting part was 81.5\%.


\section*{Conclusion}

Though powerful tools exist to organize text contents, most of them do not
match today's needs. Indeed, anyone can now generate a significant amount
of digitize data and organizing them quickly becomes essential.

Concerning our expectations, most of them have been reached. The application
works, provides a REST API, is easy to configure and works with 2 different
DBMS. As we saw in the tests we drove, the classification process speed can
still be improved. Perfecting SortDC is our next objective.

\addcontentsline{toc}{section}{Conclusion}



\clearpage
\section*{Bibliography}
\addcontentsline{toc}{section}{Bibliography}

\textbf{Machine Learning in Automated Text Categorization} \\
\url{http://arxiv.org/pdf/cs.ir/0110053} \\
~\\
\textbf{Bayes' Thoerem} \\
\url{http://en.wikipedia.org/wiki/Bayes'_theorem} \\
~\\
\textbf{Classification supervisée de documents} \\
\url{http://docs.happycoders.org/orgadoc/artifical_intelligence/classification_documents/classification.pdf} \\
~\\
\textbf{Cours Recherche d'Information - Analyse et Indexation des documents et des requêtes} \\
\url{http://www.iro.umontreal.ca/~nie/IFT6255/Indexation.html} \\
~\\
\textbf{Linguistique computationnelle : pourquoi, comment ?} \\
\url{http://www.labri.fr/perso/mery/aide-m\%C3\%A9moire-tal.pdf} \\
~\\
\textbf{RESTful} \\
\url{http://en.wikipedia.org/wiki/Representational_State_Transfer#RESTful_web_services} \\


\end{document}
